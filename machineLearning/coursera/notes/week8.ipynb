{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of clustering\n",
    "\n",
    "- Market of Clustering\n",
    "- Social network analysis\n",
    "- Organize conputing vlusters\n",
    "- Astronomical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- K(number of clusters)\n",
    "- Training set {$x^{(1)}, x^{(2)}, ..., x{(m)}$}\n",
    "\n",
    "$x^{(i)} \\in \\mathbb{R}$ (drop $x_0 = 1$ convention)\n",
    "\n",
    "Randomly initialize K cluster centroids $\\mu_1, \\mu_2, ..., \\mu_K \\in \\mathbb{R}$<br>\n",
    "Repeat{<br>\n",
    "for $i$ = 1 to $m$<br>\n",
    "$c^{(i)}$ := index (from 1 to K) of cluster centroid closest to $x^{(i)}$<br>\n",
    "($c^{(i)} = min_k||x^{(i)} - \\mu_k||^2$)\n",
    "\n",
    "for $k$ = 1 to $K$<br>\n",
    "$\\mu_k$ := average (mean) of points assigind to cluster $k$<br>\n",
    "}\n",
    "\n",
    "### K-meands for non-separated clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizatoin Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means optimization objective\n",
    "\n",
    "$c^{(i)} = index of cluster (1, 2, ..., K)$ to which example $x^{(i)}$ is currently assigned<br>\n",
    "$\\mu_k$ = cluster centroid $k\\;\\mu_k \\in \\mathbb{R}$<br>\n",
    "$\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n",
    "\n",
    "Optimization objective:<br>\n",
    "$J(x^{(1)}, ..., c^{m}, \\mu_1, ..., \\mu_K) = \\frac{1}{m}\\sum^m_{i=1}||x^{(i)} - \\mu_{c^{(i)}}||^2$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "\n",
    "Should have $K < m$\n",
    "\n",
    "Randomly pick $K$ training example\n",
    "\n",
    "Set $\\mu_1, ..., \\mu_K$ equal to these $K$ exmaples.\n",
    "\n",
    "For i = 1 to 100{<br>\n",
    "\n",
    "Randomly initialize K-means.<br>\n",
    "Run K-means. Get $c^{(1)}..., c^{(m)}, \\mu_1, ..., \\mu_K$<br>\n",
    "Conpute cost function (distortion)<br>\n",
    "$\\rightarrow J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_K)$<br>\n",
    "}\n",
    "\n",
    "Pick clustering that gave lowest cost $J(c^{(1)}, ..., c^{(m)}, \\mu_1, ..., \\mu_K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the value of K\n",
    "\n",
    "Elbow method:<br>\n",
    "<b>K = 3 is \"Elbow point\"</b>\n",
    "\n",
    "<u>but many case, \"Elbow point\" is unclear</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cost function J')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1.0, 9.0, 1.0)\n",
    "y = np.array([10, 6, 3, 2.5, 2.1, 1.8, 1.6, 1.5])\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set_xlabel(\"K(no. of clusters)\")\n",
    "ax.set_ylabel(\"Cost function J\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you're runnning K-means to get to use for some later/downstream purpose. Evaluate K-means based on a metric for how well it performs for that later purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "<b>Question1.</b><br>\n",
    "For which of the following tasks might K-means clustering be a suitable algorithm? Select all that apply.\n",
    "    - Given a database of information about your users, automatically group them into different market segments.\n",
    "    - Given sales data from a large number of products in a supermarket, figure out which products tend to form coherent groups (say are frequently purchased together) and thus should be put on the same shelf.\n",
    "    - Given historical weather records, predict the amount of rainfall tomorrow (this would be a real-valued output)\n",
    "    - Given sales data from a large number of products in a supermarket, estimate future sales for each of these products.\n",
    "    \n",
    "<b>Question2.</b><br>\n",
    "Suppose we have three cluster centroids \n",
    "$\\mu_1=$\n",
    "$\\begin{bmatrix}\n",
    "1\\\\2\n",
    "\\end{bmatrix}$, \n",
    "$\\mu_2=$\n",
    "$\\begin{bmatrix}\n",
    "-3\\\\0\n",
    "\\end{bmatrix}$, \n",
    "$\\mu_3=$\n",
    "$\\begin{bmatrix}\n",
    "4\\\\2\n",
    "\\end{bmatrix}$.\n",
    "Furthermore, we have a training example\n",
    "$x^{(i)}=$\n",
    "$\\begin{bmatrix}\n",
    "3\\\\1\n",
    "\\end{bmatrix}$.\n",
    "After a cluster assignment step, what will $c^{(i)}$ be?\n",
    "\n",
    "- $c^{(i)}$ = 1\n",
    "- $c^{(i)}$ is not assigned\n",
    "- $c^{(i)}$ = 2\n",
    "- $c^{(i)}$ = 3\n",
    "\n",
    "\n",
    "<b>Question3.</b><br>\n",
    "K-means is an iterative algorithm, and two of the following steps are repeatedly carried out in its inner-loop. Which two?\n",
    "- The cluster assignment step, where the parameters $c^{(i)}$ are updated.\n",
    "- The cluster centroid assignment step, where each cluster centroid $\\mu_i$ is assigned (by setting $c^{(i)}$\n",
    "- Move each cluster centroid $\\mu_k$, by setting it to be equal to the closest training example $x^{(i)}$\n",
    "- Move the cluster centroids, where the centroids $\\mu_k$ are updated.\n",
    "\n",
    "<b>Question4.</b><br>\n",
    "Suppose you have an unlabeled dataset $\\{x^{(1)}, \\ldots, x^{(m)}\\}$ You run K-means with 50 different random initializations, and obtain 50 different clusterings of the data. What is the recommended way for choosing which one of these 50 clusterings to use?\n",
    "- Use the elbow method.\n",
    "- Manually examine the clusterings, and pick the best one.\n",
    "- Compute the distortion function $J(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots,\\mu_k)$ and pick the one that minimizes this.\n",
    "- Plot the data and the cluster centroids, and pick the clustering that gives the most \"coherent\" cluster centroids.\n",
    "\n",
    "<b>Question5.</b><br>\n",
    "Which of the following statements are true? Select all that apply.\n",
    "- If we are worried about K-means getting stuck in bad local optima, one way to ameliorate (reduce) this problem is if we try using multiple random initializations.\n",
    "- For some datasets, the \"right\" or \"correct\" value of K (the number of clusters) can be ambiguous, and hard even for a human expert looking carefully at the data to decide.\n",
    "- The standard way of initializing K-means is setting $\\mu_1 = \\cdots = \\mu_k$ to be equal to a vector of zeros.\n",
    "- Since K-Means is an unsupervised learning algorithm, it cannot overfit the data, and thus it is always better to have as large a number of clusters as is computationally feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation I: Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation II: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce from 2-deimension to 1-dimension: Find a direction (a vector $\\mu^{(i)} \\in \\mathbb{R}^n)$ onto which to project the data so as to minimize the projection error.\n",
    "- Reduce from n-deimension to k-dimension: Find $k$ vectors $\\mu^{(1)}, \\mu{(2)}, ..., \\mu^{(k)}$ onto which to project the data, so as to minimize zhe projection error.\n",
    "\n",
    "### PCA is not linear regression\n",
    "\n",
    "||PCA|linear regression|\n",
    "|:-------------:|:------------:|:-------------:|\n",
    "|write|write|write|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessiong\n",
    "\n",
    "Training set: $x^{(1)}, x^{(2)}, ..., x^{(m)}$<br>\n",
    "Preprocessiong(<u>feature scaling/mean normalization)</u>:<br>\n",
    "$\\mu_j = \\frac{1}{m}\\sum_{i=1}^mx_j^{(i)}$<br>\n",
    "Reduce each $x_j^{(i)}$ with $x_j - \\mu_j$<br>\n",
    "If different features on different scales (e.g., $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.\n",
    "\n",
    "### Principal Component Analysis(PCA) algorithm\n",
    "Reduce data from $n$-dimensions to $k$-dimensions<br>\n",
    "Compute \"convariance matrix\":<br>\n",
    "$\\Sigma = \\frac{1}{m}\\sum_{i =1}^n(x^{(i)}(x^{(i)})^T)$<br>\n",
    "Compute \"eigenventors\" of matrix $\\Sigma$:<br>\n",
    "$[U, S, V] = svd(Sigma)$\n",
    "\n",
    "From $[U, S, V] = svd(Sigma)$, we get:<br>\n",
    "$U = \\begin{bmatrix}\n",
    "|&|&  &|\\\\\n",
    "u^{(1)}& u^{(2)}& ...& u^{(n)}\\\\\n",
    "|&|&  &|\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "### Principal Component Analysis(PCA) algorithm summary\n",
    "\n",
    "After mean normalization (ensure every feature has zero mean) and optionally feature scaling:<br>\n",
    "$Sigma = \\frac{1}{m}(x^{(i)})(x^{(i)})^T$<br>\n",
    "$[U, S, V] = svd(Sigma);$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction from Compressed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$ (number of principal components)\n",
    "\n",
    "Average squared projection error: $\\frac{1}{m}\\sum_{i=1}^m||x^{(i)} - x_{approx}^{(1)}||^2$<br>\n",
    "Total variation in the data: $\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2$\n",
    "\n",
    "Typically, choose $k$ to be smallest value so that<br>\n",
    "$\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\leq 0.01$ (1%)\n",
    "\n",
    "$\\rightarrow$ \"99% of variance is retained\"\n",
    "\n",
    "\n",
    "\n",
    "Algorithm:<br>\n",
    "Try PCA with $k = 1$<br>\n",
    "Compute $U_{reduce}, z^{(1)}, z^{(2)}, ..., z^{(m)}, x_{approx}^{(1)}, ..., x_{approx}^{(m)}$<br>\n",
    "Check if <br>\n",
    "$\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\leq 0.01$?\n",
    "\n",
    "$\\rightarrow$ [U, <u>S</u>, V] = svd(Sigma)<br>\n",
    "S = $\\begin{bmatrix}\n",
    "S_{11}&&&&&&0\\\\\n",
    "&S_{22}&&&&&\\\\\n",
    "&&S_{33}&&&&\\\\\n",
    "&&&&...&&\\\\\n",
    "0&&&&&&S_{nn}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "For given $k$:<br>\n",
    "$1 - \\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}-nS_{ii}} \\leq 0.01$<br>\n",
    "$\\rightarrow\\; \\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}-nS_{ii}} \\geq 0.99$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for Applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervises learning speedup\n",
    "\n",
    "$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$<br>\n",
    "Extract inpus:<br>\n",
    "Unlabeled dataset: <br>\n",
    "$x^{(1)}, x^{(2)}, ..., x^{(m)} \\in \\mathbb{R}^{10000}$<br>\n",
    "$\\downarrow \\; PCA$<br>\n",
    "$z^{(1)}, z^{(2)}, ..., z^{(m)} \\in \\mathbb{R}^{1000}$\n",
    "\n",
    "#### New training set:\n",
    "\n",
    "$(z^{(1)}, y^{(1)}), (z^{(2)}, y^{(2)}), ..., (z^{(m)}, y^{(m)})$<br>\n",
    "Note: Mapping $x^{(i)} \\rightarrow z^{(i)}$ should be defined by runnning PCA only on the training set. This mapping can be applied as well to the examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in the cros validation and test sets.\n",
    "\n",
    "\n",
    "### Application of PCA\n",
    "- Compression\n",
    "    - Reduce memory/disc needed to store data\n",
    "    - Speed up learning algorithm\n",
    "    \n",
    "- Visualization:\n",
    "k = 2 or 3\n",
    "\n",
    "### Bad use of PCA: To prevent overfitting\n",
    "\n",
    "Use $z^{(i)}$ instead of $x^{i}$ to reduce the number of features to $k < n$. <br>\n",
    "Thus, fewer features, less likely to overfit.\n",
    "\n",
    "This might work OK, but isn't a good way to address overfitting. Use regularization instead.<br>\n",
    "$min_\\theta \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m}\\sum_{j = 1}^n\\theta_j^2$\n",
    "\n",
    "\n",
    "### OCA is sometimes used where it shouldn't be\n",
    "\n",
    "Design of ML system:\n",
    "- Get training set $\\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}) \\}$\n",
    "- Run PCA to reduce $x^{i}$ in dimension to get $z^{(i)}$\n",
    "- Train logistic regression on $\\{ (z^{(1)}, y^{(1)}), ... (z^{(m)}, y^{(m)}) \\}$\n",
    "- Test on test set: Map $x_{test}^{(i)}$ to $z_{test}^{(i)}$. Run $h_\\theta(z)$ on $\\{ (z_{test}^{(1)}, y_{test}^{(1)}), ..., (z_{test}^{(m)}, y_{test}^{(m)})\\}$\n",
    "    \n",
    "$\\rightarrow$ How about doing the whole thing without using PCA?<br>\n",
    "$\\rightarrow$ Before implementing PCA, first try running whatever you want to do with the original/raw data $x^{(i)}$. Only if that doesn't do what you want, then implement PCA and cosider using $z^{(i)}$.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "<u><b>Question1:</b></u><br>\n",
    "Consider the following 2D dataset:<br>\n",
    "\n",
    "[graph]\n",
    "\n",
    "Which of the following figures correspond to possible values that PCA may return for $u^{(1)}$ (the first eigenvector / first principal component)? Check all that apply (you may have to check more than one figure).\n",
    "\n",
    "\n",
    "<u><b>Question2:</b></u><br>\n",
    "Which of the following is a reasonable way to select the number of principal components $k$?\n",
    "\n",
    "(Recall that nn is the dimensionality of the input data and mm is the number of input examples.)\n",
    "\n",
    "- Choose $k$ to be the smallest value so that at least 99% of the variance is retained.\n",
    "- Choose the value of kk that minimizes the approximation error $\\frac{1}{m}\\sum_{i=1}^m||x^{(i)} - x_{approx}^{(1)}||^2$\n",
    "- Choose $k$ to be the smallest value so that at least 1% of the variance is retained.\n",
    "- Choose $k$ to be 99% of nn (i.e., $k = 0.99*n$, rounded to the nearest integer).\n",
    "\n",
    "<u><b>Question3:</b></u><br>\n",
    "Suppose someone tells you that they ran PCA in such a way that \"95% of the variance was retained.\" What is an equivalent statement to this?\n",
    "- $\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\leq 0.95$\n",
    "- $\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\leq 0.05$\n",
    "- $\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\geq 0.05$\n",
    "- $\\frac{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\geq 0.95$\n",
    "\n",
    "<u><b>Question4:</b></u><br>\n",
    "Which of the following statements are true? Check all that apply.\n",
    "- PCA is susceptible to local optima; trying multiple random initializations may help.\n",
    "- Given only $z^{(i)}$ and $U_{\\rm reduce}$, there is no way to reconstruct any reasonable approximation to $x^{(i)}$\n",
    "- Given input data $x \\in \\mathbb{R}^n$, it makes sense to run PCA only with values of $k$ that satisfy $k \\le n$. (In particular, running it with $k = n$ is possible but not helpful, and $k > n$ does not make sense.)\n",
    "- Even if all the input features are on very similar scales, we should still perform mean normalization (so that each feature has zero mean) before running PCA.\n",
    "\n",
    "<u><b>Question5:</b></u><br>\n",
    "Which of the following are recommended applications of PCA? Select all that apply.\n",
    "- Data compression: Reduce the dimension of your data, so that it takes up less memory / disk space.\n",
    "- Data visualization: To take 2D data, and find a different way of plotting it in 2D (using k=2).\n",
    "- As a replacement for (or alternative to) linear regression: For most learning applications, PCA and linear regression give substantially similar results.\n",
    "- Data compression: Reduce the dimension of your input data $x^{(i)}$, which will be used in a supervised learning algorithm (i.e., use PCA so that your supervised learning algorithm runs faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
