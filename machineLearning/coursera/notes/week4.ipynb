{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## None-linear Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons and the Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Origins: Algorithms that try to mimic the brain.<br>\n",
    "Was very widely used in 80s and early 90s: popularity diminished in late 90s.<br>\n",
    "REcent resurgence: State-of-the-art technique for many appications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation I "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Model: Logistic unit\n",
    "\n",
    "$\n",
    "x = \\begin{bmatrix}\n",
    "x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\n",
    "\\end{bmatrix}\n",
    "\\theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Sigmoid(logistic): activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "$a_i^{(j)}$ = \"activation\" of unit $i$ in layer $j$<br>\n",
    "$\\Theta^{(j)}$ = matrix of weights contorolling function mapping from layer $j$ to layer $j + 1$\n",
    "\n",
    "example:<br>\n",
    "$a_ 1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$<br>\n",
    "$a_ 2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$<br>\n",
    "$a_ 3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$<br>\n",
    "$h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)})$\n",
    "\n",
    "If network has $j, s_{j+1}$ units in layer $j + 1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$\n",
    "\n",
    "$\\Theta = \\begin{bmatrix}\n",
    "\\theta_1^0 & \\theta_1^1 & \\theta_1^2 & \\theta_1^3\\\\\n",
    "\\theta_2^0 & \\theta_2^1 & \\theta_2^2 & \\theta_2^3\\\\\n",
    "\\theta_3^0 & \\theta_3^1 & \\theta_3^2 & \\theta_3^3\\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation: Vectorized implemantation\n",
    "\n",
    "$a_ 1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\rightarrow g(z_1^{(2)})$<br>\n",
    "$a_ 2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\rightarrow g(z_2^{(2)})$<br>\n",
    "$a_ 3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\rightarrow g(z_3^{(2)})$<br>\n",
    "$h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)})$\n",
    "\n",
    "\n",
    "$x = \\begin{bmatrix}\n",
    "x_0\\\\x_1\\\\x_2\\\\x_3\n",
    "\\end{bmatrix}$\n",
    "$z^{(2)} = \\begin{bmatrix}\n",
    "z_1^{(2)}\\\\z_2^{(2)}\\\\z_3^{(3)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$x^{(2)} = \\Theta^{(1)}x \\rightarrow \\Theta^{(1)}a^{(1)}$<br>\n",
    "$a^{(2)} = g(z^{(2)})$<br>\n",
    "<b>ADD</b> $a_0^{(2)} = 1$<br>\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$<br>\n",
    "$h_\\Theta(x) = a^{(3)} = g(z^{(3)})$\n",
    "\n",
    "$h_\\Theta(x) = a^{(j + 1)} = g(z^{j + 1})$\n",
    "\n",
    "<u>in this last step, between $j$ and $j+1$, <br>we are doing exactly the same thing as we did logistic regression</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example and intutions I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "x_0\\\\x_1\\\\x_2\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "[g(z^{(2)})]\n",
    "\\rightarrow\n",
    "h_\\Theta(x)\\\\\n",
    "(x_0) = 1\n",
    "$\n",
    "\n",
    "$\\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30 & 20 & 20\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$h_\\Theta(x) = g(-30 + 20x_1 + 20x_2)$\n",
    "\n",
    "|$x_1$|$x_2$|g|\n",
    "|----------|-----------|----------|\n",
    "|0|0|g(-30) = 0|\n",
    "|0|1|g(-10) = 0|\n",
    "|1|0|g(-10) = 0|\n",
    "|1|1|g(10) = 1|\n",
    "\n",
    "This is <u><b>AND gate</b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example and intutions II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "AND:\\\\\n",
    "\\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30 & 20 & 20\n",
    "\\end{bmatrix}\\\\\n",
    "NOR:\\\\\n",
    "\\Theta^{(1)} = \\begin{bmatrix}\n",
    "10 & -20 & -20\n",
    "\\end{bmatrix}\\\\\n",
    "OR:\\\\\n",
    "\\Theta^{(1)} = \\begin{bmatrix}\n",
    "-10 & 20 & 20\n",
    "\\end{bmatrix}\\\\\n",
    "$\n",
    "\n",
    "$XOR:\\\\\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\x_1\\\\x_2\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a_1^{(1)}\\\\a_2^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a^{(3)}\n",
    "\\end{bmatrix}\\\\\n",
    "\\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30&20&20\\\\\n",
    "10&-20&-20\n",
    "\\end{bmatrix}\\\\\n",
    "\\Theta^{(2)} = \\begin{bmatrix}\n",
    "-10&20&20\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple output units: One-vs-all\n",
    "\n",
    "Exaple:<br>\n",
    "$y^{(i)} = \n",
    "\\begin{bmatrix}\n",
    "1\\\\0\\\\0\\\\0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0\\\\1\\\\0\\\\0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0\\\\0\\\\1\\\\0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0\\\\0\\\\0\\\\1\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "Prdestrian\\\\Car\\\\Motorcycle\\\\Truck\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\x_1\\\\x_2\\\\...\\\\x_n\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a_0^{(2)}\\\\a_1^{(2)}\\\\a_2^{(2)}\\\\...\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a_0^{(3)}\\\\a_1^{(3)}\\\\a_2^{(3)}\\\\...\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "...\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "h_\\Theta(x)_1\\\\h_\\Theta(x)_2\\\\h_\\Theta(x)_3\\\\h_\\Theta(x)_4\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$h_\\theta(x) = \\begin{bmatrix}\n",
    "0\\\\0\\\\1\\\\0\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "Motorcycle\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "1.Question1<br>\n",
    "Which of the following statements are true? Check all that apply.\n",
    "- Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let $a_1^{(3)} = (h_\\Theta(x))_1$ be the activation of the first output unit, and similarly $a_2^{(3)} = (h_\\Theta(x))_2$ and $a_3^{(3)}$.Then for any input xx, it must be the case that $a_1^{(3)} + a_2^{(3)} + a_3^{(3)} = 1$ ã€€[seleted]\n",
    "\n",
    "- Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately) represented using some neural network.  [seleted]\n",
    "\n",
    "- The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1).  [seleted]\n",
    "\n",
    "- A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function.\n",
    "\n",
    "2.Question2<br>\n",
    "Consider the following neural network which takes two binary-valued inputs $x_1, x_2 \\in {0,1}$ and outputs $h_\\Theta(x)$. Which of the following logical functions does it (approximately) compute?\n",
    "- OR  [seleted]\n",
    "- AND\n",
    "- NAND (meaning \"NOT AND\")\n",
    "- XOR (exclude OR)\n",
    "\n",
    "3.Question3<br>\n",
    "Consider the neural network given below. Which of the following equations correctly computes the activation $a_1^{(3)}$? Note: g(z) is the sigmoid activation function.\n",
    "- $a_1^{(3)} = g(\n",
    "\\Theta^{(2)}_{1,0}a^{(2)}_0 + \n",
    "\\Theta^{(2)}_{1,1}a^{(2)}_1 + \\Theta^{(2)}_{1,2}a^{(2)}_2)$  [seleted]\n",
    "- $a_1^{(3)} = g(\n",
    "\\Theta^{(1)}_{1,0}a^{(1)}_0 + \n",
    "\\Theta^{(1)}_{1,1}a^{(1)}_1 + \\Theta^{(1)}_{1,2}a^{(1)}_2)$\n",
    "- $a_1^{(3)} = g(\n",
    "\\Theta^{(1)}_{1,0}a^{(2)}_1 + \n",
    "\\Theta^{(1)}_{1,1}a^{(2)}_1 + \\Theta^{(1)}_{1,2}a^{(2)}_2)$\n",
    "- The activation $a_1^{(3)}$ is not present in this network.\n",
    "\n",
    "\n",
    "4.Question4<br>\n",
    "You have the following neural network:\n",
    "\n",
    "You'd like to compute the activations of the hidden layer $a^{2} \\in \\mathbb{R}^3$\n",
    "\n",
    "You want to have a vectorized implementation of this (i.e., one that does not use for loops). Which of the following implementations correctly compute $a^{(2)}$\n",
    " ? Check all that apply.\n",
    " - a2 = sigmoid (Theta1 * x); [seleted]\n",
    " - a2 = sigmoid (x * Theta1);\n",
    " - a2 = sigmoid (Theta2 * x);\n",
    " - z = sigmoid(x); a2 = Theta1 * z;\n",
    "\n",
    "5.Question5<br>\n",
    "You are using the neural network pictured below and have learned the parameters $\\Theta^{(1)} = \\begin{bmatrix}1&1&2.4\\\\1&1.7&3.2\\end{bmatrix}$ (used to compute $a^{(2)}$) and $\\Theta^{(2)} = \\begin{bmatrix}1&0.3&-1.2\\end{bmatrix}$(used to compute $a^{(3)}$ as a function of $a^{(2)}$). Suppose you swap the parameters for the first hidden layer between its two units so $\\Theta^{(1)} = \\begin{bmatrix}1&1.7&3.2\\\\1&1&2.4\\end{bmatrix}$ and also swap the output layer so $\\Theta^{(2)} = \\begin{bmatrix}1&-1.2&-0.3\\end{bmatrix}$. How will this change the value of the output $h_\\Theta(x)$?\n",
    " - It will stay the same. [seleted]\n",
    " - It will increase.\n",
    " - It will decrease.\n",
    " - Insufficient information to tell: it may increase or decrease.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
