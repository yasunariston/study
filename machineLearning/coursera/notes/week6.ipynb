{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decideing What to Try Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuging a learning algorithm:\n",
    "Suppose you have implemanted regularized linear regression to predict housing prices.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\bigl[\\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum^m_{j=1}\\theta^2_j\\bigr]$\n",
    "\n",
    "However, When you test your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions. What should you try next?\n",
    "- Get more training data\n",
    "- Try smaller sets of features\n",
    "- Try getting additional features\n",
    "- Trh adding polynomial features ($x^2_1, x^2_2, x_1x_2, exc$)\n",
    "- Try decreaseing $\\lambda$\n",
    "- Try increasing $\\lambda$\n",
    "\n",
    "abobe, it is <b>non-effectionve</b> to improve machine learning machine learning\n",
    "\n",
    "### Machine learnign diagnostic:\n",
    "Diagnostic: A test that you can run gain insight what is/isn't working with a learning algorithm, and gain guidance as to how best to imporve its performance.\n",
    "\n",
    "Diagnostics can take time to implement, but doing so can be a very good use of your time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalluating your hypothesis\n",
    "A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70% of your data and the test set is the remaining 30%.\n",
    "\n",
    "### Training/testing procedire for linear regression\n",
    "- Learn parameter $\\theta$ from training data(minimizing training error $J(\\theta)$)\n",
    "- Compute test set error:<br>\n",
    "$J_{test}(\\theta) = \\frac{1}{2m_{test}}\\sum^{mtest}_{i=1}h_\\theta(X^{(i)}_{test} - y^{(i)}_{test})^2$\n",
    "\n",
    "### Training/testing procedure for logistic regression\n",
    "- Learn parameter $\\theta$ from training data\n",
    "- Compute test set error:<br>\n",
    "$J_{test}(\\theta) = -\\frac{1}{m_{test}}\\sum^{m_{test}}_{(i=1)}y^{(i)}_{test}logh_\\theta(x^{(u)}_{test}) + (1-y^{(i)}_{test})logh_\\theta(x^{(i)}_{test})$\n",
    "- Misclassification error (0/1 misclassification error):<br>\n",
    "$\n",
    "err(h^{(x)}_\\theta, y)=\\Big\\{\\begin{align}\n",
    "&1 \\; if\\; h_\\theta(x) \\geq 0.5\\; and \\; y = 0\\; or \\; h_\\theta(x)<0.5\\;and\\;y=1\\\\\n",
    "&0 \\; otherwise\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:<br>\n",
    "\n",
    "Test Error = $\\frac{1}{m_{test}}\\sum^{m_{test}}_{(i=1)}err(h_\\theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$<br>\n",
    "\n",
    "This gives us the proportion of the test data that was misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set.\n",
    "\n",
    "Given many models with different polynomial degrees, we can use a systematic approach to identify the 'best' function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.\n",
    "\n",
    "One way to break down our dataset into the three sets is:\n",
    "- Training set: 60%\n",
    "- Cross validation set: 20%\n",
    "- Test set: 20%\n",
    "\n",
    "We can now calculate three separate error values for the three different sets using the following method:\n",
    "\n",
    "1. Optimize the parameters in Θ using the training set for each polynomial degree.\n",
    "2. Find the polynomial degree d with the least error using the cross validation set.\n",
    "3. Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)}$, (d = theta from polynomial with lower error);\n",
    "\n",
    "This way, the degree of the polynomial d has not been trained using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deignosing Bias vs. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.\n",
    "\n",
    "- We need to distinguish whether bias or variance is the problem contributing to bad predictions.\n",
    "- High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.\n",
    "\n",
    "The training error will tend to decrease as we increase the degree d of the polynomial.\n",
    "\n",
    "At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve.\n",
    "\n",
    "<b>High bias(underfitting):</b> both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ will be high. Also, $J_{CV}(\\Theta)\\approx J_{train}(\\Theta)$\n",
    "\n",
    "<b>High variance(overfitting):</b> $J_{train}(\\Theta)$ will be low  and $J_{CV}(\\Theta)$ will be much greader than $J_{train}(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we see that as $\\lambda$ increases, our fit becomes more rigid. On the other hand, as $\\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\\lambda$ to get it 'just right' ? In order to choose the model and the regularization term λ, we need to:\n",
    "\n",
    "1. Create a list of lambdas (i.e. $\\lambda\\in${0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});\n",
    "2. Create a set of models with different degrees or any other variants.\n",
    "3. Iterate through the $\\lambda$s and for each $\\lambda$ go through all the models to learn some $\\Theta$.\n",
    "4. Compute the cross validation error using the learned $\\Theta$ (computed with $\\lambda$) on the $J_{CV}(\\Theta)$ without regularization or $\\lambda$ = 0.\n",
    "5. Select the best combo that produces the lowest error on the cross validation set.\n",
    "6. Using the best combo $\\Theta$ and $\\lambda$, apply it on $J_{test}(\\Theta) $ to see if it has a good generalization of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning cueves\n",
    "\n",
    "$\n",
    "J_{train}(\\theta) = \\frac{1}{2m}\\sum^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})^2\\\\\n",
    "J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum^{m_{cv}}_{i=1}(h_\\theta(x_{cv})^{(i)} - y_{cv}^{(i)})^2\n",
    "$\n",
    "\n",
    "### Experiencing high bias:\n",
    "\n",
    "<b>Low traininig set size:</b> cause $J_{train}(\\Theta)$ to be low and $J_{cv}(\\Theta)$ to be high<br>\n",
    "<b>Large traininig set size:</b> causes both $J_{train}(\\Theta)$ and $J_{cv}(\\Theta)$ to be high with $J_{train}(\\Theta) \\approx J_{cv}(\\Theta)$\n",
    "\n",
    "### Experiencing high variance:\n",
    "\n",
    "<b>Low traininig set size:</b> cause $J_{train}(\\Theta)$ will be low and $J_{cv}(\\Theta)$ will be high<br>\n",
    "<b>Large traininig set size:</b> $J_{train}(\\Theta)$ increase with training set size and $J_{cv}(\\Theta)$ continue to decrease without leveling of off. Also,  $J_{train}(\\Theta) < J_{cv}(\\Theta)$ but the defference between them remain significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding What to Do Next Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision process can be broken down as follows:\n",
    "- <b>Try smaller sets of features:</b> Fixes high variance\n",
    "- <b>Try getting additional features:</b> Fixes high variance\n",
    "- <b>Try adding polynomial features:</b> Fixes high bias\n",
    "- <b>Try decreaseing $\\lambda$:</b> Fixes high bias\n",
    "- <b>Try increasing $\\lambda$:</b> Fixes high variance\n",
    "\n",
    "### Diagnosing Neural Networks\n",
    "- A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper.\n",
    "- A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting.\n",
    "\n",
    "Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.\n",
    "\n",
    "### Model Complexity Effects:\n",
    "- Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "- Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "- In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Test\n",
    "\n",
    "<b>Question1</b>:<u>AC</u><br>\n",
    "You train a learning algorithm, and find that it has unacceptably high error on the test set. You plot the learning curve, and obtain the figure below. Is the algorithm suffering from high bias, high variance, or neither?\n",
    "- High bias\n",
    "- High variance [selected]\n",
    "- Neither\n",
    "\n",
    "<b>Question2</b>:<u>AC</u><br>\n",
    "Suppose you have implemented regularized logistic regression to classify what object is in an image (i.e., to do object recognition). However, when you test your hypothesis on a new set of images, you find that it makes unacceptably large errors with its predictions on the new images. However, your hypothesis performs well (has low error) on the training set. Which of the following are promising steps to take? Check all that apply.\n",
    "- Try decreasing the regularization parameter $\\lambda$.\n",
    "- Try increasing the regularization parameter $\\lambda$. [selected]\n",
    "- Try using a smaller set of features. [selected]\n",
    "- Try evaluating the hypothesis on a cross validation set rather than the test set.\n",
    "\n",
    "<b>Question3</b>:<u>AC</u><br>\n",
    "Suppose you have implemented regularized logistic regression to predict what items customers will purchase on a web shopping site. However, when you test your hypothesis on a new set of customers, you find that it makes unacceptably large errors in its predictions. Furthermore, the hypothesis performs poorly on the training set. Which of the following might be promising steps to take? Check all that apply.\n",
    "- Try using a smaller set of features.\n",
    "- Try to obtain and use additional features. [selected]\n",
    "- Try adding polynomial features. [selecterd]\n",
    "- Try increasing the regularization parameter $\\lambda$.\n",
    "\n",
    "<b>Question4</b>:<u>AC</u><br>\n",
    "Which of the following statements are true? Check all that apply.\n",
    "- Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\\lambda$ to use is to choose the value of $\\lambda$ which gives the lowest <b>test set</b> error.\n",
    "- Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\\lambda$ to use is to choose the value of $\\lambda$ which gives the lowest <b>cross validation</b> error. [selected]\n",
    "- Suppose you are training a regularized linear regression model.The recommended way to choose what value of regularization parameter $\\lambda$ to use is to choose the value of $\\lambda$ which gives the lowest <b>training set</b> error.\n",
    "- The performance of a learning algorithm on the training set will typically be better than its performance on the test set. [selected]\n",
    "\n",
    "<b>Question5</b>:<u>WA</u><br>\n",
    "Which of the following statements are true? Check all that apply.\n",
    "- When debugging learning algorithms, it is useful to plot a learning curve to understand if there is a high bias or high variance problem.[selected]\n",
    "- If a learning algorithm is suffering from high bias, only adding more training examples may <b>not</b> improve the test error significantly.\n",
    "- We always prefer models with high variance (over those with high bias) as they will able to better fit the training set.\n",
    "- If a learning algorithm is suffering from high variance, adding more training examples is likely to improve the test error. [selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Prioritizing What to Work On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Design Example:\n",
    "\n",
    "Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.\n",
    "\n",
    "So how could you spend your time to improve the accuracy of this classifier?\n",
    "- Collect lots of data (for example \"honeypot\" project but doesn't always work)\n",
    "- Develop sophisticated features (for example: using email header data in spam emails)\n",
    "- Develop algorithms to process your input in different ways (recognizing misspellings in spam).\n",
    "\n",
    "It is difficult to tell which of the options will be most helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended approch\n",
    "\n",
    "- Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data.\n",
    "- Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "- Error analysis: Manually examine the example(in cross validation set) that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "$m_{CV}$ = 500 example in cross validation set<br>\n",
    "Algorithm miscassifies 100 emails.<br>\n",
    "Manually example the 100 errors, and categorize them based on:<br>\n",
    "(i) What type of email it is<br>\n",
    "(ii) What cues (features) you think would have helped the algorithm classify them correctly.\n",
    "\n",
    "Pharma: 12<br>\n",
    "Replica/fake: 4<br>\n",
    "Steal passwords: 53$\\rightarrow$ need more improve<br> \n",
    "Others: 31\n",
    "\n",
    "\n",
    "### The importance of numerical evaluations\n",
    "\n",
    "It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm's performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics for Skewed Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer classification example\n",
    "\n",
    "Train logistic regression model $h_\\theta(x)$. ($y = 1$ if cancer, $y = 0$ otherwise)<br>\n",
    "Find that you got 1% error on test set.<br>\n",
    "(99% correct diagnoses)\n",
    "\n",
    "but, <b>Only 0.50% of patients have cancer</b><br>\n",
    "$\\rightarrow$ <b><u><font color=\"red\">skewed classes</font></u></b>\n",
    "\n",
    "<pre>\n",
    "function y = predictCancer(x)\n",
    "    y = 0;  %ignore x!\n",
    "return \n",
    "</pre>\n",
    "\n",
    "above case, we get <u>0.5%</u> error.\n",
    "\n",
    "\n",
    "### Prediction/Recall\n",
    "y = 1 in presence of rare class that we want to detect\n",
    "\n",
    "||||\n",
    "|:------:|:-------:|:--------:|\n",
    "|predict\\actual|1|0|\n",
    "|1|True Positive|False Positive|\n",
    "|0|False Negative|True Negative|\n",
    "\n",
    "<b>Precision</b><br>\n",
    "(Of all parients where we predicted $y = 1$, What fraction actually has cancer?)<br>\n",
    "$\\rightarrow \\frac{True\\;positive}{predicted\\;positive} = \\frac{True\\; POS}{True\\; POS +False\\;POS}$\n",
    "\n",
    "<b>Recall</b><br>\n",
    "(Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?)<br>\n",
    "$\\rightarrow \\frac{True\\;positive}{predicted\\;positive} = \\frac{True\\; POS}{True\\; POS +False\\;NEG}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade Off Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading off precision and recall\n",
    "\n",
    "Logistic regression: $0\\leq h_\\theta(x)\\leq1$<br>\n",
    "predict 1 if $h_\\theta(x) \\leq 0.5 \\rightarrow 0.7 \\rightarrow 0.9 \\rightarrow 0.3$<br>\n",
    "predict 0 if $h_\\theta(x) < 0.5 \\rightarrow 0.7 \\rightarrow 0.9 \\rightarrow 0.3$<br>\n",
    "\n",
    "Suppopse we want to predict $y = 1$(cancer) only if very confident<br>\n",
    "$\\rightarrow$<u>High precision, lower recall</u>\n",
    "\n",
    "Suppose we want to avoid missing too many case of cancer(avoid false negatives).<br>\n",
    "$\\rightarrow$<u>High recall, lower precision</u>\n",
    "\n",
    "\n",
    "### $F_1$ Score(F score)\n",
    "How to compare precision/recall numbers?\n",
    "\n",
    "||precision(P)|recall(R)|Average|$F_1$Score|\n",
    "|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "|Algorithm1|0.5|0.4|0.45|0.444|\n",
    "|Algorithm2|0.7|0.1|0.4|0.175|\n",
    "|Algorithm3|0.02|1.0|0.51|0.392|\n",
    "\n",
    "Average: $\\frac{P+R}{2}$ <u>[non good solution]</u><br>\n",
    "$F_1$Score: $2\\frac{PR}{P+R}$\n",
    "\n",
    "1. Select many threshold for cross calidation set\n",
    "2. select threshold which has max $F_1$Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large date retionable\n",
    "\n",
    "- Assume feature $x\\in\\mathbb{R}^{n + 1}$ has sufficient infomation to predict $y$ accurately.<br>\n",
    "\n",
    "Example: For breakfast I ate ___ eggs.<br>\n",
    "Counterexample: Predict housing price from only size($feet^2$) and no other features.\n",
    "\n",
    "Useful test: GIven the input x, can a human expert confidently predict y?\n",
    "\n",
    "- Use a learning algorithm with many parameters<br>\n",
    "$\\rightarrow$ Use a very large training set(unlikely to overfit)<br>\n",
    "$\\rightarrow J_{train}(\\theta) \\approx J_{test}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Test\n",
    "\n",
    "Question1:<br>\n",
    "You are working on a spam classification system using regularized logistic regression. \"Spam\" is a positive class (y = 1) and \"not spam\" is the negative class (y = 0). You have trained your classifier and there are m = 1000 examples in the cross-validation set. The chart of predicted class vs. actual class is:\n",
    "\n",
    "||Actual Class:1|Actual Class:0|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|Predicted Class: 1|85|890|\n",
    "|Predicted Class: 2|15|10|\n",
    "\n",
    "For reference:\n",
    "- Accuracy = (true positives + true negatives) / (total examples)\n",
    "- Precision = (true positives) / (true positives + false positives)\n",
    "- Recall = (true positives) / (true positives + false negatives)\n",
    "- $F_1$score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "ans: 0.09\n",
    "\n",
    "Question2:<br>\n",
    "Suppose a massive dataset is available for training a learning algorithm. Training on a lot of data is likely to give good performance when two of the following conditions hold true.\n",
    "\n",
    "Which are the two?\n",
    "\n",
    "- When we are willing to include high order polynomial features of $x$ (such as $x^2_1, x^2_2, x_1x_2, $etc)\n",
    "- The features $x$ contain sufficient information to predict $y$ accurately. (For example, one way to verify this is if a human expert on the domain can confidently predict $y$ when given only $x$).[selected]\n",
    "- We train a learning algorithm with a large number of parameters (that is able to learn/represent fairly complex functions).[selected]\n",
    "- We train a learning algorithm with a small number of parameters (that is thus unlikely to overfit).\n",
    "\n",
    "Question3:<br>\n",
    "Suppose you have trained a logistic regression classifier which is outputing $h_\\theta(x)$.<br>\n",
    "Currently, you predict 1 if $h_\\theta(x) \\leq$ threshold, and predict 0 if $h_\\theta(x)$ < threshold, where currently the threshold set to 0.5<br>\n",
    "Suppose you decrease the threshold to 0.1. Which of the following are true? Check all that apply.\n",
    "\n",
    "- The classifier is likely to now have lower precision. [selected]\n",
    "- The classifier is likely to have unchanged precision and recall, and thus the same $F_1$Score.\n",
    "- The classifier is likely to have unchanged precision and recall, but higher accuracy.\n",
    "- The classifier is likely to now have lower recall.\n",
    "\n",
    "Questoin4:<br>\n",
    "Suppose you are working on a spam classifier, where spam emails are positive examples ($y=1$) and non-spam emails are negative examples ($y=0$). You have a training set of emails in which 99% of the emails are non-spam and the other 1% is spam. Which of the following statements are true? Check all that apply.\n",
    "\n",
    "- If you always predict spam (output $y=1$),  classifier will have a recall of 100% and precision of 1%.[selected]\n",
    "- If you always predict non-spam (output $y=0$), your classifier will have a recall of 0%.[selected]\n",
    "- If you always predict non-spam (output $y=0$), your classifier will have an accuracy of 99%.[selected]\n",
    "- If you always predict spam (output $y=1$), your classifier will have a recall of 0% and precision of 99%.\n",
    "\n",
    "Question5:<br>\n",
    "Which of the following statements are true? Check all that apply.\n",
    "- After training a logistic regression classifier, you must use 0.5 as your threshold for predicting whether an example is positive or negative.\n",
    "- Using a very large training set makes it unlikely for model to overfit the training data.[selected]\n",
    "- On skewed datasets (e.g., when there are more positive examples than negative examples), accuracy is not a good measure of performance and you should instead use $F_1$ score based on the precision and recall.[selected]\n",
    "- It is a good idea to spend a lot of time collecting a large amount of data before building your first version of a learning algorithm.\n",
    "- If your model is underfitting the training set, then obtaining more data is likely to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
